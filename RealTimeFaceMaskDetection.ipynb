{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RealTimeFaceMaskDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzR2pMWs33jG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import cv2\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
        "from keras.models import Model, load_model\n",
        "from keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils import shuffle\n",
        "import imutils\n",
        "import numpy as np"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFYoa1Cp3xiq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "8fe0e5da-be8d-4b47-da27-62ead69764b8"
      },
      "source": [
        "# model = Sequential([\n",
        "#     Conv2D(100, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
        "#     MaxPooling2D(2,2),\n",
        "#     Conv2D(100, (3,3), activation='relu'),\n",
        "#     MaxPooling2D(2,2),\n",
        "#     Flatten(),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(50, activation='relu'),\n",
        "#     Dense(2, activation='softmax')\n",
        "# ])\n",
        "\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "baseModel = MobileNetV2(include_top=False, weights='imagenet',  input_shape=(224,224,3))\n",
        "\n",
        "headModel = baseModel.output\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(50, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['acc'])\n",
        "print(\"[INFO] model compiled successfully.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 0s 0us/step\n",
            "[INFO] model compiled successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iMK3Iuo4Dfz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b33b4c68-1ea2-4301-be7f-ca9ddec5eed2"
      },
      "source": [
        "def get_generator(src):\n",
        "    datagen = ImageDataGenerator(\n",
        "        rescale=1.0/255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "        )\n",
        "                                \n",
        "    generator = datagen.flow_from_directory(\n",
        "        src,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=32\n",
        "    )\n",
        "    return generator\n",
        "\n",
        "\n",
        "train_generator = get_generator('/content/Dataset/train')\n",
        "validation_generator = get_generator('/content/Dataset/test')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1315 images belonging to 2 classes.\n",
            "Found 194 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So2B5jej5v5f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = ModelCheckpoint(\n",
        "    'model2-{epoch:03d}.model',\n",
        "    monitor='val_loss',verbose=0,\n",
        "    save_best_only=True,\n",
        "    mode='auto'\n",
        "    )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1EMfxgs5ycF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "4825525d-42b9-4d1a-a8fd-e5c8510764fa"
      },
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // 32,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // 32,\n",
        "    epochs = 20,\n",
        "    callbacks=[checkpoint]\n",
        "    )"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "41/41 [==============================] - ETA: 0s - loss: 1.8169 - acc: 0.8465INFO:tensorflow:Assets written to: model2-001.model/assets\n",
            "41/41 [==============================] - 39s 961ms/step - loss: 1.8169 - acc: 0.8465 - val_loss: 0.0242 - val_acc: 0.9948\n",
            "Epoch 2/20\n",
            "41/41 [==============================] - 22s 542ms/step - loss: 0.6500 - acc: 0.9454 - val_loss: 0.2935 - val_acc: 0.9740\n",
            "Epoch 3/20\n",
            "41/41 [==============================] - 22s 537ms/step - loss: 0.3214 - acc: 0.9696 - val_loss: 0.1790 - val_acc: 0.9844\n",
            "Epoch 4/20\n",
            "41/41 [==============================] - 22s 538ms/step - loss: 0.3999 - acc: 0.9641 - val_loss: 0.0943 - val_acc: 0.9896\n",
            "Epoch 5/20\n",
            "41/41 [==============================] - 22s 544ms/step - loss: 0.2196 - acc: 0.9719 - val_loss: 0.0611 - val_acc: 0.9896\n",
            "Epoch 6/20\n",
            "41/41 [==============================] - 22s 539ms/step - loss: 0.0618 - acc: 0.9883 - val_loss: 0.0483 - val_acc: 0.9792\n",
            "Epoch 7/20\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.0844 - acc: 0.9766INFO:tensorflow:Assets written to: model2-007.model/assets\n",
            "41/41 [==============================] - 38s 933ms/step - loss: 0.0844 - acc: 0.9766 - val_loss: 0.0070 - val_acc: 1.0000\n",
            "Epoch 8/20\n",
            "41/41 [==============================] - 22s 541ms/step - loss: 0.0562 - acc: 0.9867 - val_loss: 0.0424 - val_acc: 0.9896\n",
            "Epoch 9/20\n",
            "41/41 [==============================] - 22s 533ms/step - loss: 0.0543 - acc: 0.9821 - val_loss: 0.0183 - val_acc: 0.9948\n",
            "Epoch 10/20\n",
            "41/41 [==============================] - 22s 532ms/step - loss: 0.0653 - acc: 0.9797 - val_loss: 0.0214 - val_acc: 0.9896\n",
            "Epoch 11/20\n",
            "41/41 [==============================] - ETA: 0s - loss: 0.0548 - acc: 0.9821INFO:tensorflow:Assets written to: model2-011.model/assets\n",
            "41/41 [==============================] - 38s 938ms/step - loss: 0.0548 - acc: 0.9821 - val_loss: 0.0060 - val_acc: 0.9948\n",
            "Epoch 12/20\n",
            "41/41 [==============================] - 22s 541ms/step - loss: 0.0549 - acc: 0.9790 - val_loss: 0.0144 - val_acc: 0.9948\n",
            "Epoch 13/20\n",
            "41/41 [==============================] - 22s 539ms/step - loss: 0.0548 - acc: 0.9821 - val_loss: 0.0126 - val_acc: 0.9896\n",
            "Epoch 14/20\n",
            "41/41 [==============================] - 22s 536ms/step - loss: 0.0549 - acc: 0.9891 - val_loss: 0.0860 - val_acc: 0.9896\n",
            "Epoch 15/20\n",
            "41/41 [==============================] - 22s 534ms/step - loss: 0.0780 - acc: 0.9790 - val_loss: 0.0655 - val_acc: 0.9740\n",
            "Epoch 16/20\n",
            "41/41 [==============================] - 22s 537ms/step - loss: 0.0673 - acc: 0.9751 - val_loss: 0.0066 - val_acc: 0.9948\n",
            "Epoch 17/20\n",
            "41/41 [==============================] - 22s 542ms/step - loss: 0.0758 - acc: 0.9813 - val_loss: 0.0241 - val_acc: 0.9844\n",
            "Epoch 18/20\n",
            "41/41 [==============================] - 22s 539ms/step - loss: 0.0452 - acc: 0.9867 - val_loss: 0.0129 - val_acc: 0.9896\n",
            "Epoch 19/20\n",
            "41/41 [==============================] - 22s 539ms/step - loss: 0.0813 - acc: 0.9829 - val_loss: 0.0682 - val_acc: 0.9896\n",
            "Epoch 20/20\n",
            "41/41 [==============================] - 22s 538ms/step - loss: 0.0888 - acc: 0.9813 - val_loss: 0.0244 - val_acc: 0.9896\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esghz_XrSUlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"/content/detector.model\", save_format = \"h5\")"
      ],
      "execution_count": 25,
      "outputs": []
    }
  ]
}